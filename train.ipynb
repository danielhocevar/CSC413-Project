{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import BasicTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load('X.pt')\n",
    "y = torch.load('y.pt')\n",
    "\n",
    "# Create a set of all possible play descriptors\n",
    "play_descriptors = set()\n",
    "for s in x.flatten():\n",
    "    play_descriptors.add(s)\n",
    "for s in y.flatten():\n",
    "    play_descriptors.add(s)\n",
    "\n",
    "# Assign each play descriptor a unique token\n",
    "play_to_tok = {s: i for i, s in enumerate(play_descriptors)}\n",
    "num_token_types = len(play_descriptors)\n",
    "\n",
    "# Convert x and y to their token representations\n",
    "x_tok = torch.zeros(x.shape, dtype=torch.long)\n",
    "y_tok = torch.zeros(y.shape, dtype=torch.long)\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        x_tok[i, j] = play_to_tok[x[i, j]]\n",
    "\n",
    "for i in range(len(y)):\n",
    "    y_tok[i] = play_to_tok[y[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_examples = math.floor(len(x) * 0.8)\n",
    "\n",
    "# Train on only one example for now\n",
    "x_train = x_tok[:train_examples][:1]\n",
    "y_train = y_tok[:train_examples][:1]\n",
    "\n",
    "x_test = x_tok[train_examples:]\n",
    "y_test = y_tok[train_examples:]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(x_train, y_train)),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(x_test, y_test)),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs=50):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        i = 0\n",
    "        for x_bat, y_bat in iter(train_loader):\n",
    "            x_bat.permute(1, 0)\n",
    "            # x_bat = F.one_hot(x_bat, num_classes=num_token_types).res\n",
    "            y_bat = F.one_hot(y_bat, num_classes=num_token_types)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_bat)\n",
    "            loss = criterion(y_pred, y_bat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            i += 1\n",
    "            print(f'Epoch {epoch}, iter {i}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielhocevar/miniconda3/envs/csc413/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = BasicTransformer(num_token_types, 128, 128, 2, 2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iter 1, loss: 1.6774523258209229\n",
      "Epoch 1, iter 2, loss: 1.6876333951950073\n",
      "Epoch 2, iter 3, loss: 1.699212908744812\n",
      "Epoch 3, iter 4, loss: 1.6746820211410522\n",
      "Epoch 4, iter 5, loss: 1.681522011756897\n",
      "Epoch 5, iter 6, loss: 1.6916401386260986\n",
      "Epoch 6, iter 7, loss: 1.6921319961547852\n",
      "Epoch 7, iter 8, loss: 1.6903804540634155\n",
      "Epoch 8, iter 9, loss: 1.6930173635482788\n",
      "Epoch 9, iter 10, loss: 1.6939946413040161\n",
      "Epoch 10, iter 11, loss: 1.681666374206543\n",
      "Epoch 11, iter 12, loss: 1.6895463466644287\n",
      "Epoch 12, iter 13, loss: 1.685921311378479\n",
      "Epoch 13, iter 14, loss: 1.699947476387024\n",
      "Epoch 14, iter 15, loss: 1.6905410289764404\n",
      "Epoch 15, iter 16, loss: 1.6917647123336792\n",
      "Epoch 16, iter 17, loss: 1.6950290203094482\n",
      "Epoch 17, iter 18, loss: 1.690560221672058\n",
      "Epoch 18, iter 19, loss: 1.687844157218933\n",
      "Epoch 19, iter 20, loss: 1.691892147064209\n",
      "Epoch 20, iter 21, loss: 1.6854383945465088\n",
      "Epoch 21, iter 22, loss: 1.6818132400512695\n",
      "Epoch 22, iter 23, loss: 1.6999239921569824\n",
      "Epoch 23, iter 24, loss: 1.681809663772583\n",
      "Epoch 24, iter 25, loss: 1.6862765550613403\n",
      "Epoch 25, iter 26, loss: 1.6880464553833008\n",
      "Epoch 26, iter 27, loss: 1.692845344543457\n",
      "Epoch 27, iter 28, loss: 1.689954161643982\n",
      "Epoch 28, iter 29, loss: 1.6909090280532837\n",
      "Epoch 29, iter 30, loss: 1.6964972019195557\n",
      "Epoch 30, iter 31, loss: 1.6854645013809204\n",
      "Epoch 31, iter 32, loss: 1.6870269775390625\n",
      "Epoch 32, iter 33, loss: 1.6942451000213623\n",
      "Epoch 33, iter 34, loss: 1.689019799232483\n",
      "Epoch 34, iter 35, loss: 1.6947904825210571\n",
      "Epoch 35, iter 36, loss: 1.6859058141708374\n",
      "Epoch 36, iter 37, loss: 1.6916683912277222\n",
      "Epoch 37, iter 38, loss: 1.6763523817062378\n",
      "Epoch 38, iter 39, loss: 1.690540075302124\n",
      "Epoch 39, iter 40, loss: 1.676542043685913\n",
      "Epoch 40, iter 41, loss: 1.6903094053268433\n",
      "Epoch 41, iter 42, loss: 1.6910539865493774\n",
      "Epoch 42, iter 43, loss: 1.683425784111023\n",
      "Epoch 43, iter 44, loss: 1.6840366125106812\n",
      "Epoch 44, iter 45, loss: 1.6828964948654175\n",
      "Epoch 45, iter 46, loss: 1.6874027252197266\n",
      "Epoch 46, iter 47, loss: 1.6787614822387695\n",
      "Epoch 47, iter 48, loss: 1.696834683418274\n",
      "Epoch 48, iter 49, loss: 1.6868292093276978\n",
      "Epoch 49, iter 50, loss: 1.692787766456604\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc413",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
